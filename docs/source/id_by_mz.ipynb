{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification by Accurate Mass\n",
    "\n",
    "Example workflow for the processing of a set of mzML files (defined in\n",
    "the `files` variable) including centroiding, feature detection, feature\n",
    "linking and accurate mass search. The resulting data gets processed in a\n",
    "pandas data frame with feature filtering (missing values, quality) and\n",
    "imputation of remaining missing values. Compounds detected during\n",
    "accurate mass search will be annoted in the resulting dataframe.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "from pyopenms import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input\n",
    "\n",
    "Important: `files` directory containing your mzML files\n",
    "`accurate_mass_search_params` for your compounds create custom\n",
    "`db:mapping` and `db:struct` files as well as adduct lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.path.join(os.getcwd(), 'IdByMz_Example')\n",
    "already_centroided = False\n",
    "\n",
    "accurate_mass_search_params = { # defaults in comments\n",
    "    b'ionization_mode': b'negative', # b'positive'\n",
    "    b'positive_adducts': str.encode(os.path.join(files, 'PositiveAdducts.tsv')), # b'CHEMISTRY/PositiveAdducts.tsv'\n",
    "    b'negative_adducts': str.encode(os.path.join(files, 'NegativeAdducts.tsv')), # b'CHEMISTRY/NegativeAdducts.tsv'\n",
    "    b'db:mapping': [str.encode(os.path.join(files, 'HMDBMappingFile.tsv'))], # b'CHEMISTRY/HMDBMappingFile.tsv'\n",
    "    b'db:struct': [str.encode(os.path.join(files, 'HMDB2StructMapping.tsv'))], # b'CHEMISTRY/HMDB2StructMapping.tsv'\n",
    "}\n",
    "\n",
    "allowed_missing_values = 1\n",
    "min_feature_quality = 0.8\n",
    "n_nearest_neighbours = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Example Data\n",
    "\n",
    "This cell is important only for the example workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(os.getcwd(), 'IdByMz_Example')):\n",
    "    os.mkdir(os.path.join(os.getcwd(), 'IdByMz_Example'))\n",
    "\n",
    "base = 'https://abibuilder.informatik.uni-tuebingen.de/archive/openms/Tutorials/Data/latest/Example_Data/Metabolomics/'\n",
    "urls = ['datasets/2012_02_03_PStd_050_1.mzML',\n",
    "        'datasets/2012_02_03_PStd_050_2.mzML',\n",
    "        'datasets/2012_02_03_PStd_050_3.mzML',\n",
    "        'databases/PositiveAdducts.tsv',\n",
    "        'databases/NegativeAdducts.tsv',\n",
    "        'databases/HMDBMappingFile.tsv',\n",
    "        'databases/HMDB2StructMapping.tsv']\n",
    "\n",
    "for url in urls:\n",
    "    request = requests.get(base + url, allow_redirects=True)\n",
    "    open(os.path.join(files, os.path.basename(url)), 'wb').write(request.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading mzML files and Centroiding\n",
    "\n",
    "in: MS data (files); information if already centroided\n",
    "(already_centroided)\n",
    "\n",
    "out: centroided mzML files in a subfolder 'centroid' (files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not already_centroided:\n",
    "    if os.path.exists(os.path.join(files, 'centroid')):\n",
    "        shutil.rmtree(os.path.join(files, 'centroid'))\n",
    "    os.mkdir(os.path.join(files, 'centroid'))\n",
    "\n",
    "    for file in os.listdir(files):\n",
    "\n",
    "        if file.endswith('.mzML'):\n",
    "            exp_raw = MSExperiment()\n",
    "            MzMLFile().load(os.path.join(files, file), exp_raw)\n",
    "            exp_centroid = MSExperiment()\n",
    "\n",
    "            PeakPickerHiRes().pickExperiment(exp_raw, exp_centroid)\n",
    "\n",
    "            MzMLFile().store(os.path.join(files, 'centroid', file), exp_centroid)\n",
    "\n",
    "            del exp_raw\n",
    "\n",
    "    files = os.path.join(files, 'centroid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Detection\n",
    "\n",
    "in: centroided mzML files (files)\n",
    "\n",
    "out: list with FeatureMaps (feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = []\n",
    "\n",
    "for file in os.listdir(files):\n",
    "\n",
    "    if file.endswith('.mzML'):\n",
    "        exp = MSExperiment()\n",
    "        MzMLFile().load(os.path.join(files, file), exp)\n",
    "        exp.updateRanges()\n",
    "\n",
    "        feature_finder = FeatureFinder()\n",
    "        params = feature_finder.getParameters('centroided')\n",
    "        feature_map = FeatureMap()\n",
    "\n",
    "        feature_finder.run('centroided', exp, feature_map, params, FeatureMap())\n",
    "\n",
    "        feature_map.setPrimaryMSRunPath([str.encode(file[:-5])])\n",
    "\n",
    "        feature_maps.append(feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConsensusMap with ability to export pandas DataFrames with intensity and meta values\n",
    "\n",
    "will be obsolete when implemented in pyopenms directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsensusMapDF(ConsensusMap):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_intensity_df(self):\n",
    "        labelfree = self.getExperimentType() == \"label-free\"\n",
    "        filemeta = self.getColumnHeaders()  # type: dict[int, ColumnHeader]\n",
    "        labels = list(set([header.label for header in\n",
    "                        filemeta.values()]))  # TODO could be more efficient. Do we require same channels in all files?\n",
    "        files = list(set([header.filename for header in filemeta.values()]))\n",
    "        label_to_idx = {k: v for v, k in enumerate(labels)}\n",
    "        file_to_idx = {k: v for v, k in enumerate(files)}\n",
    "\n",
    "        def gen(cmap: ConsensusMap, fun):\n",
    "            for f in cmap:\n",
    "                yield from fun(f)\n",
    "\n",
    "        if not labelfree:\n",
    "            # TODO write two functions for LF and labelled. One has only one channel, the other has only one file per CF\n",
    "            def extractRowBlocksChannelWideFileLong(f: ConsensusFeature):\n",
    "                subfeatures = f.getFeatureList()  # type: list[FeatureHandle]\n",
    "                filerows = defaultdict(lambda: [0] * len(labels))  # TODO use numpy array?\n",
    "                for fh in subfeatures:\n",
    "                    header = filemeta[fh.getMapIndex()]\n",
    "                    row = filerows[header.filename]\n",
    "                    row[label_to_idx[header.label]] = fh.getIntensity()\n",
    "                return (f.getUniqueId(), filerows)\n",
    "\n",
    "            def extractRowsChannelWideFileLong(f: ConsensusFeature):\n",
    "                uniqueid, rowdict = extractRowBlocksChannelWideFileLong(f)\n",
    "                for file, row in rowdict.items():\n",
    "                    row.append(file)\n",
    "                    yield tuple([uniqueid] + row)\n",
    "\n",
    "            if len(labels) == 1:\n",
    "                labels[0] = \"intensity\"\n",
    "            dtypes = [('id', np.dtype('uint64'))] + list(zip(labels, ['f'] * len(labels)))\n",
    "            dtypes.append(('file', 'U300'))\n",
    "            # For TMT we know that every feature can only be from one file, since feature = PSM\n",
    "            #cnt = 0\n",
    "            #for f in self:\n",
    "            #    cnt += f.size()\n",
    "\n",
    "            intyarr = np.fromiter(iter=gen(self, extractRowsChannelWideFileLong), dtype=dtypes, count=self.size())\n",
    "            return pd.DataFrame(intyarr).set_index('id')\n",
    "        else:\n",
    "            # Specialized for LabelFree which has to have only one channel\n",
    "            def extractRowBlocksChannelLongFileWideLF(f: ConsensusFeature):\n",
    "                subfeatures = f.getFeatureList()  # type: list[FeatureHandle]\n",
    "                row = [0.] * len(files)  # TODO use numpy array?\n",
    "                for fh in subfeatures:\n",
    "                    header = filemeta[fh.getMapIndex()]\n",
    "                    row[file_to_idx[header.filename]] = fh.getIntensity()\n",
    "                yield tuple([f.getUniqueId()] + row)\n",
    "\n",
    "            dtypes = [('id', np.dtype('uint64'))] + list(zip(files, ['f'] * len(files)))\n",
    "            # cnt = self.size()*len(files) # TODO for this to work, we would need to fill with NAs for CFs that do not go over all files\n",
    "            cnt = self.size()\n",
    "\n",
    "            intyarr = np.fromiter(iter=gen(self, extractRowBlocksChannelLongFileWideLF), dtype=dtypes, count=cnt)\n",
    "            return pd.DataFrame(intyarr).set_index('id')\n",
    "\n",
    "    def get_metadata_df(self):\n",
    "        def gen(cmap: ConsensusMap, fun):\n",
    "            for f in cmap:\n",
    "                yield from fun(f)\n",
    "\n",
    "        def extractMetaData(f: ConsensusFeature):\n",
    "            # subfeatures = f.getFeatureList()  # type: list[FeatureHandle]\n",
    "            pep = f.getPeptideIdentifications()  # type: list[PeptideIdentification]\n",
    "            if len(pep) != 0:\n",
    "                hits = pep[0].getHits()\n",
    "                if len(hits) != 0:\n",
    "                    besthit = hits[0]  # type: PeptideHit\n",
    "                    # TODO what else\n",
    "                    yield f.getUniqueId(), besthit.getSequence().toString(), f.getCharge(), f.getRT(), f.getMZ(), f.getQuality()\n",
    "                else:\n",
    "                    yield f.getUniqueId(), None, f.getCharge(), f.getRT(), f.getMZ(), f.getQuality()\n",
    "            else:\n",
    "                yield f.getUniqueId(), None, f.getCharge(), f.getRT(), f.getMZ(), f.getQuality()\n",
    "\n",
    "        cnt = self.size()\n",
    "\n",
    "        mddtypes = [('id', np.dtype('uint64')), ('sequence', 'U200'), ('charge', 'i4'), ('RT', np.dtype('double')), ('mz', np.dtype('double')),\n",
    "                    ('quality', 'f')]\n",
    "        mdarr = np.fromiter(iter=gen(self, extractMetaData), dtype=mddtypes, count=cnt)\n",
    "        return pd.DataFrame(mdarr).set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map Retention Time Alignment\n",
    "\n",
    "in: unaligned feature maps (feature_maps)\n",
    "\n",
    "out: feature maps aligned on the first feature map in the list\n",
    "(feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get in index of feature map with highest number of features in feature map list\n",
    "ref_index = [i[0] for i in sorted(enumerate([fm.size() for fm in feature_maps]), key=lambda x:x[1])][-1]\n",
    "\n",
    "aligner = MapAlignmentAlgorithmPoseClustering()\n",
    "\n",
    "aligner.setReference(feature_maps[ref_index])\n",
    "\n",
    "for feature_map in feature_maps[:ref_index] + feature_maps[ref_index+1:]:\n",
    "    trafo = TransformationDescription()\n",
    "    aligner.align(feature_map, trafo)\n",
    "    transformer = MapAlignmentTransformer()\n",
    "    transformer.transformRetentionTimes(feature_map, trafo, True) # store original RT as meta value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of RTs before and after alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fm in feature_maps[:ref_index] + feature_maps[ref_index+1:]:\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=[f.getMetaValue('original_RT') for f in fm],y=[f.getMZ() for f in fm],\n",
    "                            mode='markers', name='original RT'))\n",
    "    fig.add_trace(go.Scatter(x=[f.getRT() for f in fm], y=[f.getMZ() for f in fm],\n",
    "                            mode='markers', name='aligned RT'))\n",
    "\n",
    "    fig.update_layout(title = fm.getMetaValue('spectra_data')[0].decode(), xaxis_title = 'RT', yaxis_title = 'm/z')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Linking\n",
    "\n",
    "in: list with FeatureMaps (feature_maps)\n",
    "\n",
    "out: ConsensusMap (consensus_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_grouper = FeatureGroupingAlgorithmQT()\n",
    "\n",
    "consensus_map = ConsensusMapDF()\n",
    "file_descriptions = consensus_map.getColumnHeaders()\n",
    "\n",
    "for i, feature_map in enumerate(feature_maps):\n",
    "    file_description = file_descriptions.get(i, ColumnHeader())\n",
    "    file_description.filename = feature_map.getMetaValue('spectra_data')[0].decode()\n",
    "    file_description.size = feature_map.size()\n",
    "    file_description.unique_id = feature_map.getUniqueId()\n",
    "    file_descriptions[i] = file_description\n",
    "\n",
    "consensus_map.setColumnHeaders(file_descriptions)\n",
    "feature_grouper.group(feature_maps, consensus_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConsensusMap to pandas DataFrame\n",
    "\n",
    "in: ConsensusMap (consensus_map)\n",
    "\n",
    "out: DataFrame with RT, mz and quality (result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = consensus_map.get_intensity_df()\n",
    "\n",
    "meta_data = consensus_map.get_metadata_df()[['RT', 'mz', 'quality']]\n",
    "\n",
    "result_df = pd.concat([meta_data, intensities], axis=1)\n",
    "result_df.reset_index(drop=True, inplace=True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accurate Mass Search\n",
    "\n",
    "in: ConsensusMap (consensus_map)\n",
    "\n",
    "out: DataFrame with identifications (id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurate_mass_search = AccurateMassSearchEngine()\n",
    "\n",
    "params = accurate_mass_search.getParameters()\n",
    "for key, value in accurate_mass_search_params.items():\n",
    "    params.setValue(key, value)\n",
    "accurate_mass_search.setParameters(params)\n",
    "\n",
    "mztab = MzTab()\n",
    "\n",
    "accurate_mass_search.init()\n",
    "\n",
    "accurate_mass_search.run(consensus_map, mztab)\n",
    "\n",
    "MzTabFile().store(os.path.join(files, 'ids.tsv'), mztab)\n",
    "\n",
    "df = pd.read_csv(os.path.join(files, 'ids.tsv'), header=None, sep='\\n')\n",
    "df = df[0].str.split('\\t', expand=True)\n",
    "column_names = df.loc[df[0] == 'SMH']\n",
    "id_df = df.loc[df[0] == 'SML']\n",
    "id_df.columns = df.loc[df[0] == 'SMH'].iloc[0]\n",
    "id_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "os.remove(os.path.join(files, 'ids.tsv'))\n",
    "\n",
    "id_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering and Imputation\n",
    "\n",
    "in: unfiltered result DataFrame (result_df)\n",
    "\n",
    "out: features below minimum quality and with too many missing values\n",
    "removed, remaining missing values imputated with KNN algorithm\n",
    "(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features that have more then the allowed number of missing values or are below minimum feature quality\n",
    "to_drop = []\n",
    "\n",
    "for i, row in result_df.iterrows():\n",
    "    if row.isna().sum() > allowed_missing_values or row['quality'] < min_feature_quality:\n",
    "        to_drop.append(i)\n",
    "\n",
    "result_df.drop(index=result_df.index[to_drop], inplace=True)\n",
    "\n",
    "# Data imputation with KNN\n",
    "imputer = Pipeline([(\"imputer\", KNNImputer(n_neighbors=2)),\n",
    "                    (\"pandarizer\",FunctionTransformer(lambda x: pd.DataFrame(x, columns = result_df.columns)))])\n",
    "\n",
    "result_df = imputer.fit_transform(result_df)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate features with identified compounds\n",
    "\n",
    "in: result DataFrame without identifications (result_df) and\n",
    "Identifications DataFrame (id_df)\n",
    "\n",
    "out: result DataFrame with new identifications column, where compound\n",
    "names and adduct are stored \\[name : adduct\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['identifications'] = pd.Series(['' for x in range(len(result_df.index))])\n",
    "\n",
    "for rt, mz, description in zip(id_df['retention_time'], id_df['exp_mass_to_charge'], id_df['description']):\n",
    "    indices = result_df.loc[(round(result_df['mz'], 6) == round(float(mz), 6)) & (round(result_df['RT'], 6) == round(float(rt), 6))].index.tolist()\n",
    "    for index in indices:\n",
    "        result_df.loc[index,'identifications'] += description + '; '\n",
    "\n",
    "result_df.to_csv(os.path.join(files, 'result.tsv'), sep = '\\t', index = False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of consensus features with identified compounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(result_df, x=\"RT\", y=\"mz\", hover_name='identifications')\n",
    "fig.update_layout(title=\"Consensus features with identifications (hover)\")\n",
    "fig.show()"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
